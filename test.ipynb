{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 799.19 | loss  8.08 | ppl  3225.26\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 805.76 | loss  6.22 | ppl   504.82\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 892.22 | loss  5.54 | ppl   254.02\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 632.27 | loss  5.20 | ppl   180.58\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 870.03 | loss  4.74 | ppl   114.30\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 825.22 | loss  4.37 | ppl    79.02\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 772.22 | loss  4.12 | ppl    61.38\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 676.10 | loss  3.94 | ppl    51.60\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 814.53 | loss  3.69 | ppl    39.90\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 880.83 | loss  3.57 | ppl    35.68\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 919.54 | loss  3.45 | ppl    31.43\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 805.06 | loss  3.43 | ppl    30.78\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 974.66 | loss  3.37 | ppl    29.05\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 649.48 | loss  3.23 | ppl    25.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2419.46s | valid loss  2.10 | valid ppl     8.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 690.36 | loss  3.00 | ppl    20.09\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 620.05 | loss  3.00 | ppl    20.06\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 661.55 | loss  2.81 | ppl    16.69\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 639.69 | loss  2.78 | ppl    16.15\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 752.21 | loss  2.69 | ppl    14.77\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 611.00 | loss  2.72 | ppl    15.13\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 632.29 | loss  2.67 | ppl    14.50\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 707.78 | loss  2.67 | ppl    14.38\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 687.27 | loss  2.58 | ppl    13.15\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 742.29 | loss  2.61 | ppl    13.53\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 652.46 | loss  2.54 | ppl    12.67\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 599.61 | loss  2.57 | ppl    13.03\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 603.83 | loss  2.58 | ppl    13.14\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 718.53 | loss  2.47 | ppl    11.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 2025.22s | valid loss  1.47 | valid ppl     4.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 645.70 | loss  2.36 | ppl    10.62\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 671.89 | loss  2.41 | ppl    11.18\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 770.65 | loss  2.27 | ppl     9.63\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 1361.80 | loss  2.36 | ppl    10.63\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 699.54 | loss  2.34 | ppl    10.33\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 889.37 | loss  2.27 | ppl     9.69\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 746.10 | loss  2.26 | ppl     9.56\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 718.39 | loss  2.24 | ppl     9.41\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 731.17 | loss  2.38 | ppl    10.86\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 708.49 | loss  2.33 | ppl    10.24\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 738.23 | loss  2.16 | ppl     8.71\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 701.13 | loss  2.35 | ppl    10.53\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 674.39 | loss  2.23 | ppl     9.26\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 776.30 | loss  2.17 | ppl     8.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 2388.01s | valid loss  1.45 | valid ppl     4.25\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  1.41 | test ppl     4.10\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
